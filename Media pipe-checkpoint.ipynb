{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-52fbd674caeb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmax_num_hands\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     min_detection_confidence=0.5)\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m   \u001b[1;31m# Read an image, flip it around y-axis for correct handedness output (see\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;31m# above).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_list' is not defined"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# For static images:\n",
    "hands = mp_hands.Hands(\n",
    "    static_image_mode=True,\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.5)\n",
    "for idx, file in enumerate(file_list):\n",
    "  # Read an image, flip it around y-axis for correct handedness output (see\n",
    "  # above).\n",
    "  image = cv2.flip(cv2.imread(file), 1)\n",
    "  # Convert the BGR image to RGB before processing.\n",
    "  results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "  # Print handedness and draw hand landmarks on the image.\n",
    "  print('Handedness:', results.multi_handedness)\n",
    "  if not results.multi_hand_landmarks:\n",
    "    continue\n",
    "  image_hight, image_width, _ = image.shape\n",
    "  annotated_image = image.copy()\n",
    "  for hand_landmarks in results.multi_hand_landmarks:\n",
    "    print('hand_landmarks:', hand_landmarks)\n",
    "    print(\n",
    "        f'Index finger tip coordinates: (',\n",
    "        f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
    "        f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight})'\n",
    "    )\n",
    "    mp_drawing.draw_landmarks(\n",
    "        annotated_image, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "  cv2.imwrite(\n",
    "      '/tmp/annotated_image' + str(idx) + '.png', cv2.flip(annotated_image, 1))\n",
    "hands.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'mediapipe.python.solutions.drawing_utils' from 'C:\\\\Users\\\\HP\\\\AppData\\\\Roaming\\\\Python\\\\Python37\\\\site-packages\\\\mediapipe\\\\python\\\\solutions\\\\drawing_utils.py'>\n"
     ]
    }
   ],
   "source": [
    "print(mp_drawing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "hands = mp_hands.Hands(\n",
    "    min_detection_confidence=0.6, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "a = []\n",
    "while cap.isOpened():\n",
    "  success, image = cap.read()\n",
    "  if not success:\n",
    "    print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "    continue\n",
    "    \n",
    "  # Flip the image horizontally for a later selfie-view display, and convert\n",
    "  # the BGR image to RGB.\n",
    "  image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "  # To improve performance, optionally mark the image as not writeable to\n",
    "  # pass by reference.\n",
    "  image.flags.writeable = False\n",
    "  results = hands.process(image)\n",
    "  # Draw the hand annotations on the image.\n",
    "  image.flags.writeable = False\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "  if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "      a.append(hand_landmarks)\n",
    "      mp_drawing.draw_landmarks(\n",
    "          image, hand_landmarks,mp_hands.HAND_CONNECTIONS)\n",
    "  cv2.imshow('MediaPipe Hands', image)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "hands.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_holistic = mp.solutions.holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "\n",
    "holistic = mp_holistic.Holistic(\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "  success, image = cap.read()\n",
    "  if not success:\n",
    "    print(\"Ignoring empty camera frame.\")\n",
    "    # If loading a video, use 'break' instead of 'continue'.\n",
    "    continue\n",
    "\n",
    "  # Flip the image horizontally for a later selfie-view display, and convert\n",
    "  # the BGR image to RGB.\n",
    "  image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "  # To improve performance, optionally mark the image as not writeable to\n",
    "  # pass by reference.\n",
    "  image.flags.writeable = False\n",
    "  results = holistic.process(image)\n",
    "\n",
    "  # Draw landmark annotation on the image.\n",
    "  image.flags.writeable = True\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image, results.face_landmarks, mp_holistic.FACE_CONNECTIONS)\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "  mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)\n",
    "  mp_drawing.draw_landmarks(\n",
    "      image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "  cv2.imshow('MediaPipe Holistic', image)\n",
    "  if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "holistic.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = str(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'landmark {\\n  x: 0.576312780380249\\n  y: 0.831899106502533\\n  z: -0.00014136261597741395\\n}\\nlandmark {'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = pop[0:98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landmark {\n",
       "  x: 0.576312780380249\n",
       "  y: 0.831899106502533\n",
       "  z: -0.00014136261597741395\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6447431445121765\n",
       "  y: 0.778201699256897\n",
       "  z: 0.029282374307513237\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6835833191871643\n",
       "  y: 0.723852813243866\n",
       "  z: 0.019672852009534836\n",
       "}\n",
       "landmark {\n",
       "  x: 0.711972713470459\n",
       "  y: 0.6842291355133057\n",
       "  z: 0.001321979914791882\n",
       "}\n",
       "landmark {\n",
       "  x: 0.732728898525238\n",
       "  y: 0.6592382788658142\n",
       "  z: -0.015803262591362\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6391034722328186\n",
       "  y: 0.6179014444351196\n",
       "  z: -0.013161850161850452\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7047783732414246\n",
       "  y: 0.6172454953193665\n",
       "  z: -0.04528580978512764\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7413139939308167\n",
       "  y: 0.6561826467514038\n",
       "  z: -0.060470499098300934\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7640999555587769\n",
       "  y: 0.6859419345855713\n",
       "  z: -0.07055279612541199\n",
       "}\n",
       "landmark {\n",
       "  x: 0.619586706161499\n",
       "  y: 0.6236683130264282\n",
       "  z: -0.04980964958667755\n",
       "}\n",
       "landmark {\n",
       "  x: 0.706964373588562\n",
       "  y: 0.6471070647239685\n",
       "  z: -0.07960601896047592\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7444751858711243\n",
       "  y: 0.7024411559104919\n",
       "  z: -0.0833798423409462\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7632089257240295\n",
       "  y: 0.7474085092544556\n",
       "  z: -0.0908665582537651\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6095738410949707\n",
       "  y: 0.6512417197227478\n",
       "  z: -0.08460782468318939\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7002378106117249\n",
       "  y: 0.6709192395210266\n",
       "  z: -0.11918167769908905\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7375244498252869\n",
       "  y: 0.7252458930015564\n",
       "  z: -0.12015049159526825\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7566874027252197\n",
       "  y: 0.766270637512207\n",
       "  z: -0.11732131987810135\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6046851873397827\n",
       "  y: 0.6878615021705627\n",
       "  z: -0.1163424700498581\n",
       "}\n",
       "landmark {\n",
       "  x: 0.6748282313346863\n",
       "  y: 0.6943078637123108\n",
       "  z: -0.1335088014602661\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7070764303207397\n",
       "  y: 0.7275292873382568\n",
       "  z: -0.12061429768800735\n",
       "}\n",
       "landmark {\n",
       "  x: 0.7289483547210693\n",
       "  y: 0.7552488446235657\n",
       "  z: -0.11061818897724152\n",
       "}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [0,6,8,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "__enter__",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-dc10069dc44a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m with mp_face_mesh.FaceMesh(\n\u001b[0;32m      6\u001b[0m     \u001b[0mmin_detection_confidence\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     min_tracking_confidence=0.5) as face_mesh:\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misOpened\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: __enter__"
     ]
    }
   ],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "\n",
    "        # Flip the image horizontally for a later selfie-view display, and convert\n",
    "        # the BGR image to RGB.\n",
    "        image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        # Draw the face mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_face_landmarks:\n",
    "          for face_landmarks in results.multi_face_landmarks:\n",
    "            a =mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec)\n",
    "            print(a)\n",
    "        cv2.imshow('MediaPipe FaceMesh', image)\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "          break\n",
    "mp_face_mesh.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For webcam input:\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "\n",
    "      # Flip the image horizontally for a later selfie-view display, and convert\n",
    "      # the BGR image to RGB.\n",
    "      image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "      # To improve performance, optionally mark the image as not writeable to\n",
    "      # pass by reference.\n",
    "      image.flags.writeable = False\n",
    "      results = face_mesh.process(image)\n",
    "\n",
    "      # Draw landmark annotation on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "      if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "          d =mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec)\n",
    "      cv2.imshow('MediaPipe FaceMesh', image)\n",
    "      if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "face_mesh.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e983f374794d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting Keypoints for Happy\n",
      "------------------------------------\n",
      "Keypoints for Happy are collected\n",
      "************************************\n",
      "collecting Keypoints for Sad\n",
      "------------------------------------\n",
      "Keypoints for Sad are collected\n",
      "************************************\n",
      "collecting Keypoints for Neutral\n",
      "------------------------------------\n",
      "Keypoints for Neutral are collected\n",
      "************************************\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Happy\",\"Sad\",\"Neutral\"]\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "happy=[]\n",
    "sad =[]\n",
    "neutral = []\n",
    "for label in labels:\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    print(f\"collecting Keypoints for {label}\")\n",
    "    time.sleep(1)\n",
    "    for imgnum in range(1000):\n",
    "          success, image = cap.read()\n",
    "          if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "          # Flip the image horizontally for a later selfie-view display, and convert\n",
    "          # the BGR image to RGB.\n",
    "          image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "          # To improve performance, optionally mark the image as not writeable to\n",
    "          # pass by reference.\n",
    "          image.flags.writeable = False\n",
    "          results = face_mesh.process(image)\n",
    "\n",
    "          # Draw landmark annotation on the image.\n",
    "          image.flags.writeable = True\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "          if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "              a =mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "                    landmark_drawing_spec=drawing_spec,\n",
    "                    connection_drawing_spec=drawing_spec)\n",
    "              b  = []\n",
    "              if len(a)==112:\n",
    "                  for pos,i in enumerate(a):\n",
    "                      for pos1,j in enumerate(a):\n",
    "                          if pos!=pos1:\n",
    "                              x1,y1 = i\n",
    "                              x2,y2 = j\n",
    "                              b.append(cal_distance(x1,y1,x2,y2))\n",
    "                  if label==\"Happy\":\n",
    "                        happy.append(b)\n",
    "                  elif label==\"Sad\":\n",
    "                        sad.append(b)\n",
    "                  elif label==\"Neutral\":\n",
    "                        neutral.append(b)\n",
    "          cv2.putText(image,  \n",
    "                f\"collecting data for {label}\",  \n",
    "                (50, 50),  \n",
    "                 cv2.FONT_HERSHEY_SIMPLEX , 1,  \n",
    "                (0,255,0),  \n",
    "                2,  \n",
    "                cv2.LINE_4)\n",
    "          cv2.imshow('MediaPipe FaceMesh', image)\n",
    "          if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "    print(\"------------------------------------\")\n",
    "    print(f\"Keypoints for {label} are collected\")  \n",
    "    print(\"************************************\")\n",
    "    cap.release()\n",
    "face_mesh.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "1000\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(happy))\n",
    "print(len(sad))\n",
    "print(len(neutral))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12432"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(happy[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l(emo,label):\n",
    "    o =[]\n",
    "    m =[]\n",
    "    for i in emo:\n",
    "        k=[]\n",
    "        for j in i:\n",
    "            k.append(j)\n",
    "        o.append(k)\n",
    "        m.append(label)\n",
    "    return o,m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy1,happy_label = l(happy,\"happy\")\n",
    "#happy1 = np.array(happy1)\n",
    "sad1,sad_label = l(sad,\"sad\")\n",
    "#sad1= np.array(sad1)\n",
    "neutral1,neutral_label = l(neutral,\"neutral\")\n",
    "#neutral1= np.array(neutral1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(len(happy1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "happy1.extend(sad1)\n",
    "happy1.extend(neutral1)\n",
    "happy_label.extend(sad_label)\n",
    "happy_label.extend(neutral_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 3000\n"
     ]
    }
   ],
   "source": [
    "print(len(happy1),len(happy_label))\n",
    "# df =  pd.DataFrame(happy1, happy_label, \n",
    "#                columns =['key_points', 'Labels']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.DataFrame(list(zip(happy1, happy_label)), \n",
    "               columns =['key_points', 'Labels']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"facial_key_points_distance.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_points</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[8.06225774829855, 18.027756377319946, 29.8328...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[9.848857801796104, 19.72308292331602, 32.2024...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[9.433981132056603, 19.4164878389476, 30.61045...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[8.94427190999916, 18.867962264113206, 31.4006...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[9.433981132056603, 19.4164878389476, 31.40063...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          key_points Labels\n",
       "0  [8.06225774829855, 18.027756377319946, 29.8328...  happy\n",
       "1  [9.848857801796104, 19.72308292331602, 32.2024...  happy\n",
       "2  [9.433981132056603, 19.4164878389476, 30.61045...  happy\n",
       "3  [8.94427190999916, 18.867962264113206, 31.4006...  happy\n",
       "4  [9.433981132056603, 19.4164878389476, 31.40063...  happy"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv(\"facial_key_points_distance.csv\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_points</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[315, 341, 321, 318, 318, 325, 316, 293, 323, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[342, 340, 348, 315, 343, 323, 340, 291, 349, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[361, 339, 369, 315, 361, 322, 359, 291, 370, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[380, 340, 389, 315, 379, 323, 377, 291, 390, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[397, 339, 408, 315, 395, 323, 396, 292, 409, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          key_points Labels\n",
       "0  [315, 341, 321, 318, 318, 325, 316, 293, 323, ...  happy\n",
       "1  [342, 340, 348, 315, 343, 323, 340, 291, 349, ...  happy\n",
       "2  [361, 339, 369, 315, 361, 322, 359, 291, 370, ...  happy\n",
       "3  [380, 340, 389, 315, 379, 323, 377, 291, 390, ...  happy\n",
       "4  [397, 339, 408, 315, 395, 323, 396, 292, 409, ...  happy"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df2 = df2.drop(\"Unnamed: 0\",axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "df2.key_points = df2.key_points.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_points</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[315, 341, 321, 318, 318, 325, 316, 293, 323, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[342, 340, 348, 315, 343, 323, 340, 291, 349, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[361, 339, 369, 315, 361, 322, 359, 291, 370, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[380, 340, 389, 315, 379, 323, 377, 291, 390, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[397, 339, 408, 315, 395, 323, 396, 292, 409, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          key_points Labels\n",
       "0  [315, 341, 321, 318, 318, 325, 316, 293, 323, ...  happy\n",
       "1  [342, 340, 348, 315, 343, 323, 340, 291, 349, ...  happy\n",
       "2  [361, 339, 369, 315, 361, 322, 359, 291, 370, ...  happy\n",
       "3  [380, 340, 389, 315, 379, 323, 377, 291, 390, ...  happy\n",
       "4  [397, 339, 408, 315, 395, 323, 396, 292, 409, ...  happy"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.append(df,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pehle purani wali csv file delete krni ha\n",
    "df2.to_csv(\"facial_key_points.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_points</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[315, 341, 321, 318, 318, 325, 316, 293, 323, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[342, 340, 348, 315, 343, 323, 340, 291, 349, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[361, 339, 369, 315, 361, 322, 359, 291, 370, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[380, 340, 389, 315, 379, 323, 377, 291, 390, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[397, 339, 408, 315, 395, 323, 396, 292, 409, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          key_points Labels\n",
       "0  [315, 341, 321, 318, 318, 325, 316, 293, 323, ...  happy\n",
       "1  [342, 340, 348, 315, 343, 323, 340, 291, 349, ...  happy\n",
       "2  [361, 339, 369, 315, 361, 322, 359, 291, 370, ...  happy\n",
       "3  [380, 340, 389, 315, 379, 323, 377, 291, 390, ...  happy\n",
       "4  [397, 339, 408, 315, 395, 323, 396, 292, 409, ...  happy"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [[1,2,3,4,5,6],[7,8,9,10,11,12]]\n",
    "# a = np.array(a)\n",
    "# b=  [\"a\",\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df  = pd.DataFrame(list(zip(a,b)),columns=[\"Key Points\",\"Labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key_points</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[315, 341, 321, 318, 318, 325, 316, 293, 323, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[342, 340, 348, 315, 343, 323, 340, 291, 349, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[361, 339, 369, 315, 361, 322, 359, 291, 370, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[380, 340, 389, 315, 379, 323, 377, 291, 390, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[397, 339, 408, 315, 395, 323, 396, 292, 409, ...</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          key_points Labels\n",
       "0  [315, 341, 321, 318, 318, 325, 316, 293, 323, ...  happy\n",
       "1  [342, 340, 348, 315, 343, 323, 340, 291, 349, ...  happy\n",
       "2  [361, 339, 369, 315, 361, 322, 359, 291, 370, ...  happy\n",
       "3  [380, 340, 389, 315, 379, 323, 377, 291, 390, ...  happy\n",
       "4  [397, 339, 408, 315, 395, 323, 396, 292, 409, ...  happy"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array(df[\"key_points\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "array=np.stack(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[315, 341, 321, ..., 280, 373, 277],\n",
       "       [342, 340, 348, ..., 275, 386, 273],\n",
       "       [361, 339, 369, ..., 272, 395, 270],\n",
       "       ...,\n",
       "       [227, 338, 228, ..., 228, 321, 225],\n",
       "       [228, 338, 229, ..., 228, 322, 225],\n",
       "       [229, 335, 229, ..., 227, 322, 224]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Labels\"]\n",
    "y =np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[315, 341, 321, ..., 280, 373, 277],\n",
       "       [342, 340, 348, ..., 275, 386, 273],\n",
       "       [361, 339, 369, ..., 272, 395, 270],\n",
       "       ...,\n",
       "       [227, 338, 228, ..., 228, 321, 225],\n",
       "       [228, 338, 229, ..., 228, 322, 225],\n",
       "       [229, 335, 229, ..., 227, 322, 224]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = [\"A\",\"B\",\"C\"]\n",
    "b = [1,2,3]\n",
    "a,b = [item for item in [a,b]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import time\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        print(\"Ignoring empty camera frame.\")\n",
    "        # If loading a video, use 'break' instead of 'continue'.\n",
    "        continue\n",
    "\n",
    "      # Flip the image horizontally for a later selfie-view display, and convert\n",
    "      # the BGR image to RGB.\n",
    "      image = cv2.cvtColor(cv2.flip(image, 1), cv2.COLOR_BGR2RGB)\n",
    "      # To improve performance, optionally mark the image as not writeable to\n",
    "      # pass by reference.\n",
    "      image.flags.writeable = False\n",
    "      results = face_mesh.process(image)\n",
    "\n",
    "      # Draw landmark annotation on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "      if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:s\n",
    "          d =mp_drawing.draw_landmarks(\n",
    "                image=image,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACE_CONNECTIONS,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec)\n",
    "      cv2.imshow('MediaPipe FaceMesh', image)\n",
    "      if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "face_mesh.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47213595499958\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "x1,y1 = (2,4)\n",
    "x2,y2 = (4,8)\n",
    "def cal_distance(x1,y1,x2,y2):\n",
    "    dist = math.sqrt((x2-x1)**2+(y2-y1)**2)\n",
    "    return dist\n",
    "a = cal_distance(x1,y1,x2,y2)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-8d7261cc5f1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mct\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[0mpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFONT_HERSHEY_PLAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "import cv2 \n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "mp_drawing  = mp.solutions.drawing_utils\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1,circle_radius=1)\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5,min_tracking_confidence=0.5)\n",
    "cap = cv2.VideoCapture(0)\n",
    "ct = 0\n",
    "pt = 0\n",
    "points = []\n",
    "while True:\n",
    "    succes,img = cap.read()\n",
    "    imgRGB = cv2.cvtColor(cv2.flip(img,1),cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(img)\n",
    "    if results.multi_face_landmarks:\n",
    "        for lm in results.multi_face_landmarks:\n",
    "            for id1,lm_id in enumerate(lm.landmark):\n",
    "                h,w,c = img.shape\n",
    "                cx,cy = int(lm_id.x*w),int(lm_id.y*h)\n",
    "                #cv2.putText(img,str(id1),(cx,cy),cv2.FONT_HERSHEY_PLAIN,0.7,(0,0,255))\n",
    "            a= mp_drawing.draw_landmarks(img,lm,mp_face_mesh.FACE_CONNECTIONS)\n",
    "            b  = []\n",
    "            for pos,i in enumerate(a):\n",
    "                for pos1,j in enumerate(a):\n",
    "                    if pos!=pos1:\n",
    "                        x1,y1 = i\n",
    "                        x2,y2 = j\n",
    "                        b.append(cal_distance(x1,y1,x2,y2))\n",
    "                        cv2.line(img,i,j,(255,0,0))\n",
    "            points.append(b)\n",
    "                        \n",
    "    ct = time.time()\n",
    "    if ct==0:\n",
    "        ct=1\n",
    "    fp = 1/(float(ct)-float(pt))\n",
    "    pt = ct\n",
    "    cv2.putText(img,str(int(fp)),(10,70),cv2.FONT_HERSHEY_PLAIN,1,(0,255,0),3)\n",
    "    cv2.imshow(\"Face\",img)\n",
    "    if cv2.waitKey(1)==ord(\"q\"):\n",
    "        break\n",
    "face_mesh.close()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance as dist\n",
    "a = [1,2,3,4]\n",
    "b = []\n",
    "for pos,i in enumerate(a):\n",
    "    for pos1,j in enumerate(a):\n",
    "        if pos!=pos1:\n",
    "            c = dist.cdist(i,j,metrics=\"euclidean\")\n",
    "            b.append(c)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12432"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "112*111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12432\n"
     ]
    }
   ],
   "source": [
    "print(len(points[40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
